================================================================================
GENETIC ALGORITHM BATCH EXPERIMENT REPORT
================================================================================
Model: meta-llama/Llama-3.2-1B-Instruct
Generated: 2025-07-15 16:13:34
Total Experiments: 8

SUMMARY STATISTICS
----------------------------------------
Average valid solutions per scenario: 18.8
Best overall reduction achieved: 99.9%
Unique effective tokens discovered: 19
Total successful combinations: 43

TOP INDIVIDUAL TOKENS
----------------------------------------
 1. Token ID 106920
    Frequency: 48, Avg Reduction: 14.2%
    Effective in 2/8 scenarios (25.0%)

 2. Token ID 127865
    Frequency: 40, Avg Reduction: 1.2%
    Effective in 1/8 scenarios (12.5%)

 3. Token ID 127896
    Frequency: 35, Avg Reduction: 55.4%
    Effective in 2/8 scenarios (25.0%)

 4. Token ID 120454
    Frequency: 32, Avg Reduction: 94.6%
    Effective in 1/8 scenarios (12.5%)

 5. Token ID 110024
    Frequency: 28, Avg Reduction: 86.9%
    Effective in 1/8 scenarios (12.5%)

 6. Token ID 117691
    Frequency: 27, Avg Reduction: 99.2%
    Effective in 1/8 scenarios (12.5%)

 7. Token ID 120325
    Frequency: 25, Avg Reduction: 88.7%
    Effective in 1/8 scenarios (12.5%)

 8. Token ID 114185
    Frequency: 25, Avg Reduction: 96.3%
    Effective in 1/8 scenarios (12.5%)

 9. Token ID 117949
    Frequency: 24, Avg Reduction: 83.3%
    Effective in 1/8 scenarios (12.5%)

10. Token ID 111067
    Frequency: 18, Avg Reduction: 93.4%
    Effective in 1/8 scenarios (12.5%)

TOP TOKEN COMBINATIONS
----------------------------------------
 1. Tokens: [106920, 127711, 106920]
    Frequency: 18, Avg Reduction: 16.2%
    Effective in 1/8 scenarios (12.5%)

 2. Tokens: [120454, 120454, 110024]
    Frequency: 13, Avg Reduction: 99.9%
    Effective in 1/8 scenarios (12.5%)

 3. Tokens: [120325, 126173, 111067]
    Frequency: 12, Avg Reduction: 98.4%
    Effective in 1/8 scenarios (12.5%)

 4. Tokens: [124292, 114185, 113924]
    Frequency: 11, Avg Reduction: 98.6%
    Effective in 1/8 scenarios (12.5%)

 5. Tokens: [114091, 117691, 115721]
    Frequency: 10, Avg Reduction: 99.4%
    Effective in 1/8 scenarios (12.5%)

 6. Tokens: [117949, 127896, 115487]
    Frequency: 10, Avg Reduction: 97.7%
    Effective in 1/8 scenarios (12.5%)

 7. Tokens: [127896, 113983, 106920]
    Frequency: 9, Avg Reduction: 10.6%
    Effective in 1/8 scenarios (12.5%)

 8. Tokens: [127865]
    Frequency: 7, Avg Reduction: 4.2%
    Effective in 1/8 scenarios (12.5%)

 9. Tokens: [127865, 127865, 127865]
    Frequency: 7, Avg Reduction: 0.3%
    Effective in 1/8 scenarios (12.5%)

10. Tokens: [127865, 127865]
    Frequency: 6, Avg Reduction: 1.0%
    Effective in 1/8 scenarios (12.5%)

SCENARIO DIFFICULTY ANALYSIS
----------------------------------------
Scenario: question_start
  Base text: 'What is the'
  Baseline probability: 0.0651
  Valid solutions found: 20
  Best reduction: 98.6%
  Difficulty score: 0.00

Scenario: coding_context
  Base text: 'def function_name('
  Baseline probability: 0.0777
  Valid solutions found: 20
  Best reduction: 99.4%
  Difficulty score: 0.00

Scenario: weak_prediction_weather
  Base text: 'The weather is'
  Baseline probability: 0.0789
  Valid solutions found: 20
  Best reduction: 98.4%
  Difficulty score: 0.00

Scenario: weak_prediction_think
  Base text: 'I think that'
  Baseline probability: 0.2896
  Valid solutions found: 20
  Best reduction: 99.9%
  Difficulty score: 0.00

Scenario: incomplete_sentence
  Base text: 'The cat sat on the'
  Baseline probability: 0.5381
  Valid solutions found: 20
  Best reduction: 97.7%
  Difficulty score: 0.01

Scenario: math_context
  Base text: '2 + 2 ='
  Baseline probability: 0.3889
  Valid solutions found: 18
  Best reduction: 16.2%
  Difficulty score: 0.02

Scenario: strong_prediction_fox
  Base text: 'The quick brown'
  Baseline probability: 0.9487
  Valid solutions found: 12
  Best reduction: 10.6%
  Difficulty score: 0.09

Scenario: conversation_start
  Base text: 'Hello, how are'
  Baseline probability: 0.9863
  Valid solutions found: 20
  Best reduction: 4.2%
  Difficulty score: 0.24

INDIVIDUAL EXPERIMENT RESULTS
----------------------------------------
Experiment: strong_prediction_fox
  Base text: 'The quick brown'
  Target: ' fox' (ID: 39935)
  Baseline probability: 0.9487
  Best fitness: 0.1001
  Top 3 results:
    1. Tokens: [127896, 113983, 106920]
       Reduction: 10.6% (0.9487 → 0.8486)
    2. Tokens: [127896, 113983, 106920]
       Reduction: 10.6% (0.9487 → 0.8486)
    3. Tokens: [127896, 113983, 106920]
       Reduction: 10.6% (0.9487 → 0.8486)

Experiment: weak_prediction_weather
  Base text: 'The weather is'
  Target: ' getting' (ID: 3794)
  Baseline probability: 0.0789
  Best fitness: 0.0777
  Top 3 results:
    1. Tokens: [120325, 126173, 111067]
       Reduction: 98.4% (0.0789 → 0.0013)
    2. Tokens: [120325, 126173, 111067]
       Reduction: 98.4% (0.0789 → 0.0013)
    3. Tokens: [120325, 126173, 111067]
       Reduction: 98.4% (0.0789 → 0.0013)

Experiment: weak_prediction_think
  Base text: 'I think that'
  Target: ''s' (ID: 596)
  Baseline probability: 0.2896
  Best fitness: 0.2893
  Top 3 results:
    1. Tokens: [120454, 120454, 110024]
       Reduction: 99.9% (0.2896 → 0.0002)
    2. Tokens: [120454, 120454, 110024]
       Reduction: 99.9% (0.2896 → 0.0002)
    3. Tokens: [120454, 120454, 110024]
       Reduction: 99.9% (0.2896 → 0.0002)

Experiment: coding_context
  Base text: 'def function_name('
  Target: 'phrase' (ID: 28810)
  Baseline probability: 0.0777
  Best fitness: 0.0772
  Top 3 results:
    1. Tokens: [114091, 117691, 115721]
       Reduction: 99.4% (0.0777 → 0.0005)
    2. Tokens: [114091, 117691, 115721]
       Reduction: 99.4% (0.0777 → 0.0005)
    3. Tokens: [114091, 117691, 115721]
       Reduction: 99.4% (0.0777 → 0.0005)

Experiment: conversation_start
  Base text: 'Hello, how are'
  Target: ' you' (ID: 499)
  Baseline probability: 0.9863
  Best fitness: 0.0410
  Top 3 results:
    1. Tokens: [127865]
       Reduction: 4.2% (0.9863 → 0.9453)
    2. Tokens: [127865]
       Reduction: 4.2% (0.9863 → 0.9453)
    3. Tokens: [127865]
       Reduction: 4.2% (0.9863 → 0.9453)

Experiment: question_start
  Base text: 'What is the'
  Target: ' value' (ID: 907)
  Baseline probability: 0.0651
  Best fitness: 0.0642
  Top 3 results:
    1. Tokens: [124292, 114185, 113924]
       Reduction: 98.6% (0.0651 → 0.0009)
    2. Tokens: [124292, 114185, 113924]
       Reduction: 98.6% (0.0651 → 0.0009)
    3. Tokens: [124292, 114185, 113924]
       Reduction: 98.6% (0.0651 → 0.0009)

Experiment: math_context
  Base text: '2 + 2 ='
  Target: ' ' (ID: 220)
  Baseline probability: 0.3889
  Best fitness: 0.0630
  Top 3 results:
    1. Tokens: [106920, 127711, 106920]
       Reduction: 16.2% (0.3889 → 0.3259)
    2. Tokens: [106920, 127711, 106920]
       Reduction: 16.2% (0.3889 → 0.3259)
    3. Tokens: [106920, 127711, 106920]
       Reduction: 16.2% (0.3889 → 0.3259)

Experiment: incomplete_sentence
  Base text: 'The cat sat on the'
  Target: ' windows' (ID: 11276)
  Baseline probability: 0.5381
  Best fitness: 0.5257
  Top 3 results:
    1. Tokens: [117949, 127896, 115487]
       Reduction: 97.7% (0.5381 → 0.0123)
    2. Tokens: [117949, 127896, 115487]
       Reduction: 97.7% (0.5381 → 0.0123)
    3. Tokens: [117949, 127896, 115487]
       Reduction: 97.7% (0.5381 → 0.0123)
