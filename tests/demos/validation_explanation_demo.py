#!/usr/bin/env python3
"""
Comprehensive Validation Explanation and Demo Script

This script demonstrates how glitcher's validation systems work to detect
glitch tokens and avoid false positives. It shows the difference between
standard and enhanced validation methods.

Author: Glitcher Development Team
"""

import torch
import json
import sys
import os
from typing import List, Dict, Any, Tuple

# Add the glitcher package to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

from glitcher.model import (
    initialize_model_and_tokenizer,
    get_template_for_model,
    strictly_glitch_verify,
    glitch_verify_message1,
    glitch_verify_message2,
    glitch_verify_message3
)
from glitcher.enhanced_validation import enhanced_glitch_verify

def explain_validation_concepts():
    """Explain the core concepts of validation in glitcher"""
    print("=" * 80)
    print("GLITCHER VALIDATION SYSTEM EXPLANATION")
    print("=" * 80)

    print("""
üéØ WHAT ARE GLITCH TOKENS?
Glitch tokens are tokens in a language model's vocabulary that cause unexpected
behavior when the model encounters them. They often:
- Produce incoherent or random output
- Break the model's normal reasoning patterns
- Generate repetitive or nonsensical text
- Cause the model to ignore instructions

üîç WHY DO WE NEED VALIDATION?
Simple entropy-based mining can produce many false positives - tokens that
appear to be glitches but actually behave normally in real usage. Validation
systems help distinguish true glitch tokens from false positives.

üìä KEY METRICS:
‚Ä¢ ASR (Attack Success Rate): Percentage of attempts where token exhibits glitch behavior
‚Ä¢ Probability Threshold: Minimum probability for considering a token "normal"
‚Ä¢ Content Filtering: Pattern-based filtering for known false positive types

üé™ TWO VALIDATION METHODS:

1Ô∏è‚É£  STANDARD VALIDATION (strictly_glitch_verify):
   ‚Ä¢ Uses immediate next-token prediction
   ‚Ä¢ Checks 3 different prompt formats
   ‚Ä¢ Very strict probability thresholds (0.00001 - 0.00005)
   ‚Ä¢ Binary result: glitch or not glitch
   ‚Ä¢ Fast but can miss some edge cases

2Ô∏è‚É£  ENHANCED VALIDATION (enhanced_glitch_verify):
   ‚Ä¢ Generates full text sequences (up to max_tokens)
   ‚Ä¢ Searches for target token in generated text
   ‚Ä¢ Uses ASR across multiple attempts
   ‚Ä¢ Handles non-deterministic model behavior
   ‚Ä¢ More thorough but slower
""")

def explain_asr_system():
    """Explain the Attack Success Rate (ASR) system"""
    print("\n" + "=" * 80)
    print("ATTACK SUCCESS RATE (ASR) SYSTEM")
    print("=" * 80)

    print("""
üéØ ASR DEFINITION:
ASR = (Number of attempts showing glitch behavior) / (Total attempts) √ó 100%

üîÑ HOW IT WORKS:
1. Run validation multiple times (num_attempts parameter)
2. Count how many attempts show glitch behavior
3. Calculate ASR percentage
4. Compare against threshold to make final decision

üìà ASR THRESHOLDS:
‚Ä¢ ASR = 1.0 (100%): Token ALWAYS exhibits glitch behavior (strictest)
‚Ä¢ ASR = 0.8 (80%):  Token exhibits glitch behavior 80%+ of time (high confidence)
‚Ä¢ ASR = 0.5 (50%):  Token exhibits glitch behavior 50%+ of time (balanced, default)
‚Ä¢ ASR = 0.3 (30%):  Token exhibits glitch behavior 30%+ of time (lenient)
‚Ä¢ ASR = 0.0 (0%):   Any glitch behavior detected (most permissive)

üí° EXAMPLE SCENARIOS:

Token tested 5 times, glitch behavior detected in 3 attempts:
‚Ä¢ ASR = 3/5 = 0.6 (60%)
‚Ä¢ With --asr-threshold 0.5: Token classified as GLITCH ‚úì
‚Ä¢ With --asr-threshold 0.7: Token classified as NORMAL ‚úó
‚Ä¢ With --asr-threshold 1.0: Token classified as NORMAL ‚úó

üé™ WHY USE ASR?
Language models can be non-deterministic, especially with:
‚Ä¢ Temperature > 0
‚Ä¢ Different random seeds
‚Ä¢ Model quantization effects
‚Ä¢ Hardware variations

ASR accounts for this variability by requiring consistent glitch behavior
across multiple attempts rather than relying on a single test.
""")

def explain_false_positive_detection():
    """Explain false positive detection mechanisms"""
    print("\n" + "=" * 80)
    print("FALSE POSITIVE DETECTION MECHANISMS")
    print("=" * 80)

    print("""
üö´ WHAT ARE FALSE POSITIVES?
Tokens that appear to be glitches in initial screening but actually behave
normally when properly tested. Common causes:
‚Ä¢ Probability calculation artifacts
‚Ä¢ Unusual but valid tokens (programming terms, foreign characters)
‚Ä¢ Context-dependent behavior
‚Ä¢ Tokenization edge cases

üõ°Ô∏è DETECTION MECHANISMS:

1Ô∏è‚É£  CONTENT-BASED FILTERING:
   Automatically skip tokens with known false positive patterns:
   ‚Ä¢ Brackets: '[', ']', '(', ')'
   ‚Ä¢ Programming symbols: '_', '$', '.'
   ‚Ä¢ Common prefixes: 'arg', 'prop', 'char'

   Example false positives caught:
   ‚Ä¢ '[INST]' - instruction formatting token
   ‚Ä¢ 'args' - programming parameter
   ‚Ä¢ '$var' - variable reference

2Ô∏è‚É£  PROBABILITY THRESHOLDS:
   Very strict thresholds for immediate next-token probability:
   ‚Ä¢ Llama 3.2: 0.00001 (0.001%)
   ‚Ä¢ Other models: 0.00005 (0.005%)

   If token has higher probability than threshold ‚Üí likely not a glitch

3Ô∏è‚É£  MULTIPLE TEST FORMATS:
   Three different prompt formats to test token behavior:
   ‚Ä¢ Format 1: Direct completion
   ‚Ä¢ Format 2: Conversational format
   ‚Ä¢ Format 3: Instruction format

   Token must fail ALL formats to be considered a glitch

4Ô∏è‚É£  SEQUENCE GENERATION:
   Enhanced validation generates full text sequences and searches for:
   ‚Ä¢ Target token appearing in generated sequence
   ‚Ä¢ Token text appearing in generated text
   ‚Ä¢ Coherent vs incoherent output patterns

üéØ VALIDATION LOGIC:

Standard Validation Decision Tree:
‚îú‚îÄ Content filtering: Skip known false positive patterns
‚îú‚îÄ Probability check: If prob > threshold ‚Üí NOT glitch
‚îú‚îÄ Top token check: If token is top prediction ‚Üí NOT glitch
‚îî‚îÄ Multiple formats: Must fail ALL formats ‚Üí GLITCH

Enhanced Validation Decision Tree:
‚îú‚îÄ Content filtering: Same as standard
‚îú‚îÄ Generate sequences: Create text with target token as context
‚îú‚îÄ Search sequences: Look for token in generated text
‚îú‚îÄ Multiple attempts: Repeat process num_attempts times
‚îú‚îÄ Calculate ASR: Count glitch attempts / total attempts
‚îî‚îÄ Threshold check: ASR >= threshold ‚Üí GLITCH
""")

def demonstrate_validation_step_by_step(model, tokenizer, token_id: int, chat_template):
    """Demonstrate validation process step by step"""
    print(f"\n" + "=" * 80)
    print(f"STEP-BY-STEP VALIDATION DEMO: Token ID {token_id}")
    print("=" * 80)

    # Decode token
    token = tokenizer.decode([token_id])
    print(f"üéØ Target Token: '{token}' (ID: {token_id})")

    print(f"\nüìù STEP 1: GENERATE TEST PROMPTS")
    prompt1 = glitch_verify_message1(chat_template, token)
    prompt2 = glitch_verify_message2(chat_template, token)
    prompt3 = glitch_verify_message3(chat_template, token)

    print(f"   Format 1 (50 chars): '{prompt1[:50]}...'")
    print(f"   Format 2 (50 chars): '{prompt2[:50]}...'")
    print(f"   Format 3 (50 chars): '{prompt3[:50]}...'")

    print(f"\nüîç STEP 2: CONTENT-BASED FILTERING")
    # Check content filtering
    should_skip = (
        ('[' in token or ']' in token) or
        ('(' in token or ')' in token) or
        ('_' in token) or
        ('arg' in token.lower()) or
        ('prop' in token.lower()) or
        ('char' in token.lower()) or
        (token.startswith('.')) or
        (token.startswith('$'))
    )

    if should_skip:
        print(f"   ‚ùå FILTERED: Token contains false positive patterns")
        print(f"   üèÅ RESULT: NOT A GLITCH (filtered)")
        return False, 0.0
    else:
        print(f"   ‚úÖ PASSED: No false positive patterns detected")

    print(f"\nüìä STEP 3: STANDARD VALIDATION")
    is_glitch_standard = strictly_glitch_verify(model, tokenizer, token_id, chat_template)
    print(f"   Standard result: {'GLITCH' if is_glitch_standard else 'NOT GLITCH'}")

    print(f"\nüöÄ STEP 4: ENHANCED VALIDATION")
    print(f"   Running 3 attempts with ASR threshold 0.5...")
    is_glitch_enhanced, asr = enhanced_glitch_verify(
        model, tokenizer, token_id, chat_template,
        max_tokens=20, num_attempts=3, asr_threshold=0.5, quiet=True
    )

    print(f"   Enhanced result: {'GLITCH' if is_glitch_enhanced else 'NOT GLITCH'}")
    print(f"   ASR: {asr:.2%} ({'‚â•' if asr >= 0.5 else '<'} 50% threshold)")

    print(f"\nüèÅ FINAL COMPARISON:")
    print(f"   Standard:  {'‚úì GLITCH' if is_glitch_standard else '‚úó NOT GLITCH'}")
    print(f"   Enhanced:  {'‚úì GLITCH' if is_glitch_enhanced else '‚úó NOT GLITCH'} (ASR: {asr:.1%})")

    if is_glitch_standard != is_glitch_enhanced:
        print(f"   ‚ö†Ô∏è  DISAGREEMENT: Methods produced different results!")
        print(f"   This shows why enhanced validation with ASR is more reliable.")
    else:
        print(f"   ‚úÖ AGREEMENT: Both methods agree on classification.")

    return is_glitch_enhanced, asr

def run_comprehensive_demo(model_path: str, test_tokens: List[int]):
    """Run comprehensive validation demo"""
    print("Loading model for comprehensive validation demo...")

    # Load model with int4 quantization for memory efficiency
    model, tokenizer = initialize_model_and_tokenizer(model_path, quant_type="int4")
    chat_template = get_template_for_model(model_path, tokenizer)

    print(f"‚úì Model loaded: {type(model).__name__}")
    print(f"‚úì Template: {chat_template.template_name if hasattr(chat_template, 'template_name') else 'Unknown'}")

    # Run conceptual explanations
    explain_validation_concepts()
    explain_asr_system()
    explain_false_positive_detection()

    # Demo validation on test tokens
    print(f"\n" + "=" * 80)
    print("LIVE VALIDATION DEMONSTRATIONS")
    print("=" * 80)

    results = []
    for i, token_id in enumerate(test_tokens):
        try:
            is_glitch, asr = demonstrate_validation_step_by_step(
                model, tokenizer, token_id, chat_template
            )
            results.append({
                "token_id": token_id,
                "token": tokenizer.decode([token_id]),
                "is_glitch": is_glitch,
                "asr": asr
            })

            if i < len(test_tokens) - 1:
                input("\nPress Enter to continue to next token...")

        except Exception as e:
            print(f"‚ùå Error testing token {token_id}: {e}")
            continue

    # Summary
    print(f"\n" + "=" * 80)
    print("VALIDATION DEMO SUMMARY")
    print("=" * 80)

    glitch_count = sum(1 for r in results if r["is_glitch"])
    total_count = len(results)

    print(f"üìä Results: {glitch_count}/{total_count} tokens classified as glitches")
    print(f"\nDetailed Results:")
    for r in results:
        status = "GLITCH" if r["is_glitch"] else "NORMAL"
        print(f"   Token {r['token_id']} ('{r['token']}'): {status} (ASR: {r['asr']:.1%})")

    print(f"\nüí° KEY TAKEAWAYS:")
    print(f"‚Ä¢ Enhanced validation uses ASR to handle non-deterministic behavior")
    print(f"‚Ä¢ False positive filtering prevents common misclassifications")
    print(f"‚Ä¢ Multiple test formats ensure robust validation")
    print(f"‚Ä¢ ASR thresholds allow tuning sensitivity vs. specificity")

    # Save results
    results_file = "validation_demo_results.json"
    with open(results_file, 'w') as f:
        json.dump({
            "model_path": model_path,
            "test_tokens": test_tokens,
            "results": results,
            "summary": {
                "total_tested": total_count,
                "glitches_found": glitch_count,
                "glitch_rate": glitch_count / total_count if total_count > 0 else 0
            }
        }, f, indent=2)

    print(f"\nüìÅ Results saved to: {results_file}")

def demonstrate_asr_thresholds(model_path: str, token_id: int):
    """Demonstrate how different ASR thresholds affect classification"""
    print(f"\n" + "=" * 80)
    print(f"ASR THRESHOLD DEMONSTRATION")
    print("=" * 80)

    model, tokenizer = initialize_model_and_tokenizer(model_path, quant_type="int4")
    chat_template = get_template_for_model(model_path, tokenizer)

    token = tokenizer.decode([token_id])
    print(f"üéØ Testing Token: '{token}' (ID: {token_id})")

    # Test with different thresholds
    thresholds = [0.0, 0.3, 0.5, 0.7, 0.8, 1.0]
    num_attempts = 5

    print(f"üìä Running {num_attempts} attempts with different ASR thresholds:")

    # Get base ASR by running enhanced validation once
    _, base_asr = enhanced_glitch_verify(
        model, tokenizer, token_id, chat_template,
        max_tokens=20, num_attempts=num_attempts, asr_threshold=0.5, quiet=True
    )

    print(f"\nüé≤ Token achieved ASR of {base_asr:.1%} across {num_attempts} attempts")
    print(f"\nüìà Classification with different thresholds:")

    for threshold in thresholds:
        is_glitch = base_asr >= threshold
        status = "GLITCH" if is_glitch else "NORMAL"
        symbol = "‚úì" if is_glitch else "‚úó"
        print(f"   Threshold {threshold:.0%}: {symbol} {status}")

    print(f"\nüí° INTERPRETATION:")
    if base_asr == 1.0:
        print(f"‚Ä¢ Token exhibits glitch behavior in ALL attempts (100% ASR)")
        print(f"‚Ä¢ High-confidence glitch token")
    elif base_asr >= 0.8:
        print(f"‚Ä¢ Token exhibits glitch behavior in most attempts ({base_asr:.0%} ASR)")
        print(f"‚Ä¢ Likely glitch token")
    elif base_asr >= 0.5:
        print(f"‚Ä¢ Token exhibits glitch behavior in some attempts ({base_asr:.0%} ASR)")
        print(f"‚Ä¢ Possible glitch token (default threshold)")
    elif base_asr > 0.0:
        print(f"‚Ä¢ Token exhibits glitch behavior occasionally ({base_asr:.0%} ASR)")
        print(f"‚Ä¢ Likely normal token with some edge cases")
    else:
        print(f"‚Ä¢ Token never exhibits glitch behavior (0% ASR)")
        print(f"‚Ä¢ Normal token")

    print(f"\nüéõÔ∏è THRESHOLD RECOMMENDATIONS:")
    print(f"‚Ä¢ Research/Academic: Use 0.8-1.0 for high confidence")
    print(f"‚Ä¢ General Discovery: Use 0.5 (default) for balanced detection")
    print(f"‚Ä¢ Broad Screening: Use 0.3-0.4 for comprehensive analysis")
    print(f"‚Ä¢ Non-deterministic Models: Use lower thresholds with more attempts")

def main():
    """Main demonstration function"""
    # Configuration
    MIXTRAL_MODEL_PATH = os.environ.get("GLITCHER_MODEL_PATH", "meta-llama/Llama-3.2-1B-Instruct")
    TEST_TOKENS = [1000, 2000, 3000, 5000, 10000]  # Sample tokens for testing

    print("üé™ GLITCHER VALIDATION SYSTEM COMPREHENSIVE DEMO")
    print("=" * 80)
    print(f"Model: {MIXTRAL_MODEL_PATH}")
    print(f"Test Tokens: {TEST_TOKENS}")

    try:
        # Run main demo
        run_comprehensive_demo(MIXTRAL_MODEL_PATH, TEST_TOKENS)

        # Demo ASR thresholds on one token
        print(f"\nüéØ Running ASR threshold demo on token {TEST_TOKENS[0]}...")
        demonstrate_asr_thresholds(MIXTRAL_MODEL_PATH, TEST_TOKENS[0])

        print(f"\nüéâ Demo completed successfully!")
        print(f"Check the generated files for detailed results and logs.")

    except KeyboardInterrupt:
        print(f"\n‚ùå Demo interrupted by user")
        return 1

    except Exception as e:
        print(f"‚ùå Demo failed: {e}")
        return 1

    return 0

if __name__ == "__main__":
    sys.exit(main())
