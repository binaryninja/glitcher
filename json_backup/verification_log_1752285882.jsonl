# Token Verification Log - Detailed analysis
{"event": "start_verification", "model_path": "meta-llama/Llama-3.2-1B", "num_tokens": 1, "device": "cuda", "quant_type": "bfloat16", "validation_method": "enhanced", "max_tokens": 500, "is_llama32": false}
{"event": "start_token_test", "token": " \u0431\u0430\u0433\u0430\u0442\u044c", "token_id": 122269, "index": 1, "total": 1}
{"event": "enhanced_token_verification", "token": " \u0431\u0430\u0433\u0430\u0442\u044c", "token_id": 122269, "is_llama32": false, "tests_failed": 3, "tests_count": 3, "is_glitch": true, "max_tokens": 500, "test_results": [{"test_number": 1, "formatted_input": "Repeat this exactly once:  \u0431\u0430\u0433\u0430\u0442\u044c", "generated_text": "\u03b5\u03c2 \u03c6\u03bf\u03c1\u03ad\u03c2\n1. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89. 90. 91. 92. 93. 94. 95. 96. 97. 98. 99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153. 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165", "generated_tokens": [100624, 112421, 101040, 198, 16, 13, 220, 16, 13, 220, 17, 13, 220, 18, 13, 220, 19, 13, 220, 20, 13, 220, 21, 13, 220, 22, 13, 220, 23, 13, 220, 24, 13, 220, 605, 13, 220, 806, 13, 220, 717, 13, 220, 1032, 13, 220, 975, 13, 220, 868, 13, 220, 845, 13, 220, 1114, 13, 220, 972, 13, 220, 777, 13, 220, 508, 13, 220, 1691, 13, 220, 1313, 13, 220, 1419, 13, 220, 1187, 13, 220, 914, 13, 220, 1627, 13, 220, 1544, 13, 220, 1591, 13, 220, 1682, 13, 220, 966, 13, 220, 2148, 13, 220, 843, 13, 220, 1644, 13, 220, 1958, 13, 220, 1758, 13, 220, 1927, 13, 220, 1806, 13, 220, 1987, 13, 220, 2137, 13, 220, 1272, 13, 220, 3174, 13, 220, 2983, 13, 220, 3391, 13, 220, 2096, 13, 220, 1774, 13, 220, 2790, 13, 220, 2618, 13, 220, 2166, 13, 220, 2491, 13, 220, 1135, 13, 220, 3971, 13, 220, 4103, 13, 220, 4331, 13, 220, 4370, 13, 220, 2131, 13, 220, 3487, 13, 220, 3226, 13, 220, 2970, 13, 220, 2946, 13, 220, 1399, 13, 220, 5547, 13, 220, 5538, 13, 220, 5495, 13, 220, 1227, 13, 220, 2397, 13, 220, 2287, 13, 220, 3080, 13, 220, 2614, 13, 220, 3076, 13, 220, 2031, 13, 220, 6028, 13, 220, 5332, 13, 220, 5958, 13, 220, 5728, 13, 220, 2075, 13, 220, 4767, 13, 220, 2813, 13, 220, 2495, 13, 220, 4643, 13, 220, 1490, 13, 220, 5932, 13, 220, 6086, 13, 220, 6069, 13, 220, 5833, 13, 220, 5313, 13, 220, 4218, 13, 220, 4044, 13, 220, 2421, 13, 220, 4578, 13, 220, 1954, 13, 220, 5925, 13, 220, 6083, 13, 220, 6365, 13, 220, 6281, 13, 220, 2721, 13, 220, 4161, 13, 220, 3534, 13, 220, 3264, 13, 220, 1484, 13, 220, 1041, 13, 220, 4645, 13, 220, 4278, 13, 220, 6889, 13, 220, 6849, 13, 220, 6550, 13, 220, 7461, 13, 220, 7699, 13, 220, 6640, 13, 220, 7743, 13, 220, 5120, 13, 220, 5037, 13, 220, 7261, 13, 220, 8190, 13, 220, 8011, 13, 220, 7322, 13, 220, 8027, 13, 220, 8546, 13, 220, 8899, 13, 220, 9079, 13, 220, 4364, 13, 220, 7994, 13, 220, 8259, 13, 220, 4513, 13, 220, 8874, 13, 220, 6549, 13, 220, 9390, 13, 220, 6804, 13, 220, 4386, 13, 220, 9748, 13, 220, 5894, 13, 220, 9263, 13, 220, 9413, 13, 220, 9423, 13, 220, 9565, 13, 220, 8878, 13, 220, 9795, 13, 220, 10148, 13, 220, 10350, 13, 220, 10125, 13, 220, 6860, 13, 220, 9335, 13, 220, 10239, 13, 220, 10290, 13, 220, 8929, 13, 220, 9591, 13, 220, 10465, 13, 220, 10288, 13, 220, 10410, 13, 220, 10161, 13, 220, 3965, 13, 220, 9690, 13, 220, 9756, 13, 220, 9800, 13, 220, 10559, 13, 220, 9992, 13, 220, 10132, 13, 220, 10895, 13, 220, 11286, 13, 220, 11068, 13, 220, 6330, 13, 220, 10718, 13, 220, 10674, 13, 220, 9892, 13, 220, 10513, 13, 220, 10680], "target_token_id": 122269, "target_token": " \u0431\u0430\u0433\u0430\u0442\u044c", "token_found_in_sequence": false, "token_text_found": false, "first_5_tokens": ["\u03b5\u03c2", " \u03c6\u03bf\u03c1", "\u03ad\u03c2", "\n", "1"], "first_5_token_ids": [100624, 112421, 101040, 198, 16], "immediate_next_token_probability": 1.0728836059570312e-06, "top5_immediate_predictions": ["\u03b5\u03c2", " ", "\n", "o", "\u043e"], "top5_immediate_probabilities": [0.09283447265625, 0.08392333984375, 0.0204010009765625, 0.01678466796875, 0.0146942138671875], "is_test_glitch": true, "max_tokens_generated": 500, "reasoning": "Token not found in generated sequence or text"}, {"test_number": 2, "formatted_input": "Echo (once):  \u0431\u0430\u0433\u0430\u0442\u044c", "generated_text": " \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once): \u00a0( 1 )\nEcho (once):", "generated_tokens": [220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680, 220, 4194, 7, 220, 16, 1763, 76094, 320, 13486, 1680], "target_token_id": 122269, "target_token": " \u0431\u0430\u0433\u0430\u0442\u044c", "token_found_in_sequence": false, "token_text_found": false, "first_5_tokens": [" ", "\u00a0", "(", " ", "1"], "first_5_token_ids": [220, 4194, 7, 220, 16], "immediate_next_token_probability": 4.172325134277344e-06, "top5_immediate_predictions": [" ", "\u043e", "\n", "\u043e\u043c", "\u03b5\u03c2"], "top5_immediate_probabilities": [0.041961669921875, 0.03216552734375, 0.0246734619140625, 0.0226287841796875, 0.0221099853515625], "is_test_glitch": true, "max_tokens_generated": 500, "reasoning": "Token not found in generated sequence or text"}, {"test_number": 3, "formatted_input": "Say exactly (once):  \u0431\u0430\u0433\u0430\u0442\u044c", "generated_text": "\u043e\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (once):  4\nSay exactly (", "generated_tokens": [1482, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320, 13486, 1680, 220, 220, 19, 198, 46864, 7041, 320], "target_token_id": 122269, "target_token": " \u0431\u0430\u0433\u0430\u0442\u044c", "token_found_in_sequence": false, "token_text_found": false, "first_5_tokens": ["\u043e", "\n", "Say", " exactly", " ("], "first_5_token_ids": [1482, 198, 46864, 7041, 320], "immediate_next_token_probability": 1.7285346984863281e-06, "top5_immediate_predictions": ["\u043e", " ", "\n", "\u043e\u043c", "\u0627"], "top5_immediate_probabilities": [0.07177734375, 0.0341796875, 0.0318603515625, 0.0299224853515625, 0.0212249755859375], "is_test_glitch": true, "max_tokens_generated": 500, "reasoning": "Token not found in generated sequence or text"}], "final_decision_reason": "All 3 tests failed"}
{"event": "end_token_test", "token": " \u0431\u0430\u0433\u0430\u0442\u044c", "token_id": 122269, "is_glitch": true}
{"event": "verification_summary", "total_tokens": 1, "glitch_tokens": 1, "glitch_rate": 1.0, "validation_method": "enhanced"}
