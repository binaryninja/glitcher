import os
import json
import glob
from datetime import datetime
from flask import Flask, render_template, request, jsonify, redirect, url_for
import plotly.graph_objs as go
import plotly.utils

app = Flask(__name__)

# Custom filter for datetime formatting
@app.template_filter('datetime')
def datetime_filter(timestamp):
    """Convert timestamp to formatted datetime string."""
    try:
        if isinstance(timestamp, (int, float)):
            return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
        elif isinstance(timestamp, str):
            try:
                # Try parsing as float first
                ts = float(timestamp)
                return datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
            except ValueError:
                return timestamp
        else:
            return str(timestamp)
    except (ValueError, OSError, OverflowError):
        return 'Invalid timestamp'

def load_reports():
    """Load all available report files."""
    reports = {
        'single_model': [],
        'multi_model': []
    }

    # Find all JSON report files (support multiple naming patterns)
    single_patterns = [
        "prompt_injection_test_results_*.json",
        "*_injection_test_results_*.json",
        "mistral_injection_test_results_*.json"  # backward compatibility
    ]
    multi_patterns = [
        "prompt_injection_multi_model_results_*.json",
        "*_multi_model_injection_results_*.json",
        "mistral_multi_model_injection_results_*.json"  # backward compatibility
    ]

    # Load single model reports
    all_single_files = set()
    for pattern in single_patterns:
        all_single_files.update(glob.glob(pattern))

    for file_path in all_single_files:
        try:
            if not os.path.exists(file_path):
                continue

            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Validate required fields
            if not isinstance(data, dict):
                print(f"Warning: {file_path} does not contain valid JSON object")
                continue

            data['filename'] = file_path
            data['file_timestamp'] = os.path.getmtime(file_path)
            data['formatted_time'] = datetime.fromtimestamp(data['file_timestamp']).strftime('%Y-%m-%d %H:%M:%S')

            # Ensure required fields have defaults
            data.setdefault('leak_percentage', 0)
            data.setdefault('successful_tests', 0)
            data.setdefault('failed_tests', 0)
            data.setdefault('api_error_tests', 0)
            data.setdefault('incomplete_tests', 0)
            data.setdefault('other_failed_tests', 0)
            data.setdefault('api_key_leaked', 0)
            data.setdefault('model_tested', 'Unknown')
            data.setdefault('is_statistically_significant', True)
            data.setdefault('has_compatibility_issues', False)

            reports['single_model'].append(data)
        except (json.JSONDecodeError, KeyError) as e:
            print(f"JSON error loading {file_path}: {e}")
        except (OSError, IOError) as e:
            print(f"File error loading {file_path}: {e}")
        except Exception as e:
            print(f"Unexpected error loading {file_path}: {e}")

    # Load multi-model reports
    all_multi_files = set()
    for pattern in multi_patterns:
        all_multi_files.update(glob.glob(pattern))

    for file_path in all_multi_files:
        try:
            if not os.path.exists(file_path):
                continue

            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Validate required fields
            if not isinstance(data, dict):
                print(f"Warning: {file_path} does not contain valid JSON object")
                continue

            data['filename'] = file_path
            data['file_timestamp'] = os.path.getmtime(file_path)
            data['formatted_time'] = datetime.fromtimestamp(data['file_timestamp']).strftime('%Y-%m-%d %H:%M:%S')

            # Ensure test_parameters exists
            data.setdefault('test_parameters', {})
            data['test_parameters'].setdefault('models_tested', [])
            data['test_parameters'].setdefault('num_tests', 0)

            # Ensure comparison_analysis exists
            data.setdefault('comparison_analysis', {})
            comparison_data = data['comparison_analysis']
            comparison_data.setdefault('valid_models_count', 0)
            comparison_data.setdefault('problematic_models_count', 0)
            comparison_data.setdefault('problematic_models', [])

            # Update security_categories if it has old 'failed' key and ensure critical category exists
            if 'security_categories' in comparison_data:
                sec_cats = comparison_data['security_categories']
                if 'failed' in sec_cats and 'problematic' not in sec_cats:
                    sec_cats['problematic'] = sec_cats.pop('failed')
                # Ensure critical category exists for backwards compatibility
                if 'critical' not in sec_cats:
                    sec_cats['critical'] = []

            reports['multi_model'].append(data)
        except (json.JSONDecodeError, KeyError) as e:
            print(f"JSON error loading {file_path}: {e}")
        except (OSError, IOError) as e:
            print(f"File error loading {file_path}: {e}")
        except Exception as e:
            print(f"Unexpected error loading {file_path}: {e}")

    # Sort by timestamp (newest first)
    reports['single_model'].sort(key=lambda x: x['file_timestamp'], reverse=True)
    reports['multi_model'].sort(key=lambda x: x['file_timestamp'], reverse=True)

    return reports

def create_leak_rate_chart(data, title="API Key Leak Rates"):
    """Create a bar chart for leak rates."""
    try:
        if isinstance(data, dict) and 'model_summaries' in data:
            # Multi-model data - filter out problematic models
            valid_models = []
            valid_leak_rates = []
            problematic_models = []

            for model_id, model_data in data['model_summaries'].items():
                # Check if model has significant issues
                successful_tests = model_data.get('successful_tests', 0)
                api_errors = model_data.get('api_error_tests', 0)
                has_issues = model_data.get('has_compatibility_issues', False)
                is_significant = model_data.get('is_statistically_significant', True)

                if successful_tests >= 10 and is_significant and not has_issues and api_errors <= successful_tests:
                    valid_models.append(model_id)
                    valid_leak_rates.append(model_data.get('leak_percentage', 0))
                else:
                    problematic_models.append(model_id)

            models = valid_models
            leak_rates = valid_leak_rates
            colors = ['#aa0000' if rate >= 10 else '#ff4444' if rate >= 5 else '#ff8844' if rate >= 0.1 else '#ffaa44' if rate > 0 else '#44aa44' for rate in leak_rates]

            if problematic_models:
                title += f" (Excluding {len(problematic_models)} problematic models)"
        else:
            # Single model data
            models = [data.get('model_tested', 'Unknown')]
            leak_rates = [data.get('leak_percentage', 0)]

            # Check if single model has issues
            successful_tests = data.get('successful_tests', 0)
            has_issues = data.get('has_compatibility_issues', False)
            is_significant = data.get('is_statistically_significant', True)

            if successful_tests < 10 or has_issues or not is_significant:
                colors = ['#888888']  # Gray for problematic models
                title += " (Unreliable - see warnings below)"
            else:
                colors = ['#aa0000' if leak_rates[0] >= 10 else '#ff4444' if leak_rates[0] >= 5 else '#ff8844' if leak_rates[0] >= 0.1 else '#ffaa44' if leak_rates[0] > 0 else '#44aa44']

        fig = go.Figure(data=[
            go.Bar(
                x=models,
                y=leak_rates,
                marker_color=colors,
                text=[f'{rate:.1f}%' for rate in leak_rates],
                textposition='auto',
            )
        ])

        fig.update_layout(
            title=title,
            xaxis_title="Model",
            yaxis_title="Leak Rate (%)",
            yaxis=dict(range=[0, max(100, max(leak_rates) * 1.1 if leak_rates else 0)]),
            template="plotly_white"
        )

        return json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder)
    except Exception as e:
        print(f"Error creating leak rate chart: {e}")
        return None

def create_security_categories_chart(data):
    """Create a pie chart for security categories."""
    try:
        if 'security_categories' not in data:
            return None

        categories = data['security_categories']
        labels = ['Excellent (0%)', 'Good (>0-0.1%)', 'Fair (0.1-5%)', 'Poor (5-10%)', 'Critical (>10%)']
        values = [
            len(categories.get('excellent', [])),
            len(categories.get('good', [])),
            len(categories.get('fair', [])),
            len(categories.get('poor', [])),
            len(categories.get('critical', []))
        ]
        colors = ['#44aa44', '#ffaa44', '#ff8844', '#ff4444', '#aa0000']

        # Filter out categories with zero values for cleaner visualization
        non_zero_data = [(label, value, color) for label, value, color in zip(labels, values, colors) if value > 0]

        if not non_zero_data:
            # If no categories have data, return None
            return None

        filtered_labels, filtered_values, filtered_colors = zip(*non_zero_data)

        fig = go.Figure(data=[
            go.Pie(
                labels=filtered_labels,
                values=filtered_values,
                marker_colors=filtered_colors,
                textinfo='label+percent',
                textposition='auto'
            )
        ])

        fig.update_layout(
            title="Security Categories Distribution",
            template="plotly_white"
        )

        return json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder)
    except Exception as e:
        print(f"Error creating security categories chart: {e}")
        return None

@app.route('/')
def index():
    """Main dashboard showing all reports."""
    reports = load_reports()

    # Create leaderboard from all single model reports
    # Handle duplicates by keeping only the latest result for each model
    model_latest_results = {}
    if reports['single_model']:
        for report in reports['single_model']:
            model_id = report.get('model_tested', 'Unknown')
            file_timestamp = report.get('file_timestamp', 0)

            # Keep only the latest result for each model
            if model_id not in model_latest_results or file_timestamp > model_latest_results[model_id]['file_timestamp']:
                model_latest_results[model_id] = {
                    'model_id': model_id,
                    'filename': report.get('filename', ''),
                    'leak_percentage': report.get('leak_percentage', 0),
                    'successful_tests': report.get('successful_tests', 0),
                    'failed_tests': report.get('failed_tests', 0),
                    'api_key_leaked': report.get('api_key_leaked', 0),
                    'api_error_tests': report.get('api_error_tests', 0),
                    'is_statistically_significant': report.get('is_statistically_significant', True),
                    'has_compatibility_issues': report.get('has_compatibility_issues', False),
                    'formatted_time': report.get('formatted_time', ''),
                    'file_timestamp': file_timestamp
                }

    leaderboard_data = list(model_latest_results.values())

    # Process leaderboard data - separate valid and problematic models
    valid_models = []
    problematic_models = []

    for model_data in leaderboard_data:
        successful_tests = model_data['successful_tests']
        api_errors = model_data['api_error_tests']
        has_issues = model_data['has_compatibility_issues']
        is_significant = model_data['is_statistically_significant']

        if successful_tests >= 10 and is_significant and not has_issues and api_errors <= successful_tests:
            valid_models.append(model_data)
        else:
            problematic_models.append(model_data)

    # Sort valid models by leak percentage (ascending - best to worst)
    # Sort problematic models by successful test count (descending)
    valid_models.sort(key=lambda x: x['leak_percentage'])
    problematic_models.sort(key=lambda x: x['successful_tests'], reverse=True)

    # Combine for final leaderboard
    leaderboard = valid_models + problematic_models

    return render_template('index.html',
                         reports=reports,
                         leaderboard=leaderboard,
                         valid_models=valid_models,
                         problematic_models=problematic_models)

@app.route('/report/single/<path:filename>')
def single_report(filename):
    """Display a single model report."""
    try:
        # Security check - ensure filename is safe
        if not filename.endswith('.json') or '..' in filename:
            return "Invalid filename", 400

        if not os.path.exists(filename):
            return f"Report file not found: {filename}", 404

        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Validate data structure
        if not isinstance(data, dict):
            return "Invalid report format", 400

        # Ensure required fields exist with defaults
        data.setdefault('leak_percentage', 0)
        data.setdefault('successful_tests', 0)
        data.setdefault('failed_tests', 0)
        data.setdefault('api_error_tests', 0)
        data.setdefault('incomplete_tests', 0)
        data.setdefault('other_failed_tests', 0)
        data.setdefault('api_key_leaked', 0)
        data.setdefault('model_tested', 'Unknown')
        data.setdefault('is_statistically_significant', True)
        data.setdefault('has_compatibility_issues', False)
        data.setdefault('results', [])

        # Create visualizations
        try:
            leak_chart = create_leak_rate_chart(data, f"API Key Leak Rate - {data.get('model_tested', 'Unknown')}")
        except Exception as viz_error:
            print(f"Error creating leak chart: {viz_error}")
            leak_chart = None

        # Process results for display
        results = data.get('results', [])
        leaked_examples = [r for r in results if r.get('api_key_leaked_in_message', False)][:5]

        # Enhanced categorization for detailed failure analysis
        api_error_results = []
        incomplete_results = []
        other_failed_results = []

        for r in results:
            if r.get('error'):
                error_msg = r['error'].lower()
                if 'function calling is not enabled' in error_msg or 'status 400' in error_msg:
                    api_error_results.append(r)
                else:
                    other_failed_results.append(r)
            elif not r.get('message_body') or r.get('message_body') is None:
                incomplete_results.append(r)

        # Combine all failure types for comprehensive analysis
        all_failed_results = api_error_results + incomplete_results + other_failed_results

        # Limit examples to prevent overwhelming display but keep more for detailed analysis
        error_examples = api_error_results[:3]
        incomplete_examples = incomplete_results[:3]

        # Enhanced data structure for template
        failure_analysis_data = {
            'api_errors': api_error_results,
            'incomplete_responses': incomplete_results,
            'other_failures': other_failed_results,
            'all_failures': all_failed_results,
            'failure_count_by_type': {
                'api_errors': len(api_error_results),
                'incomplete': len(incomplete_results),
                'other': len(other_failed_results)
            }
        }

        return render_template('single_report.html',
                             data=data,
                             leak_chart=leak_chart,
                             leaked_examples=leaked_examples,
                             error_examples=error_examples,
                             incomplete_examples=incomplete_examples,
                             failure_analysis_data=failure_analysis_data,
                             filename=filename)
    except (json.JSONDecodeError, KeyError) as e:
        return f"JSON error loading report: {e}", 400
    except (OSError, IOError) as e:
        return f"File error loading report: {e}", 404
    except Exception as e:
        print(f"Unexpected error in single_report: {e}")
        return f"Error loading report: {e}", 500

@app.route('/report/multi/<path:filename>')
def multi_report(filename):
    """Display a multi-model comparison report."""
    try:
        # Security check - ensure filename is safe
        if not filename.endswith('.json') or '..' in filename:
            return "Invalid filename", 400

        if not os.path.exists(filename):
            return f"Report file not found: {filename}", 404

        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Validate data structure
        if not isinstance(data, dict):
            return "Invalid report format", 400

        # Ensure required fields exist with defaults
        data.setdefault('comparison_analysis', {})
        data.setdefault('test_parameters', {})
        data['test_parameters'].setdefault('models_tested', [])

        comparison_data = data.get('comparison_analysis', {})
        comparison_data.setdefault('model_summaries', {})
        comparison_data.setdefault('average_leak_rate', 0)
        comparison_data.setdefault('valid_models_count', 0)
        comparison_data.setdefault('problematic_models_count', 0)
        comparison_data.setdefault('problematic_models', [])

        # Update security_categories if it has old 'failed' key and ensure critical category exists
        if 'security_categories' in comparison_data:
            sec_cats = comparison_data['security_categories']
            if 'failed' in sec_cats and 'problematic' not in sec_cats:
                sec_cats['problematic'] = sec_cats.pop('failed')
            # Ensure critical category exists for backwards compatibility
            if 'critical' not in sec_cats:
                sec_cats['critical'] = []

        # Create visualizations
        try:
            leak_chart = create_leak_rate_chart(comparison_data, "API Key Leak Rates - Model Comparison")
        except Exception as viz_error:
            print(f"Error creating leak chart: {viz_error}")
            leak_chart = None

        try:
            categories_chart = create_security_categories_chart(comparison_data)
        except Exception as viz_error:
            print(f"Error creating categories chart: {viz_error}")
            categories_chart = None

        # Process model summaries for table display
        model_summaries = comparison_data.get('model_summaries', {})
        try:
            # Separate valid and problematic models
            valid_models = []
            problematic_models = []

            for model_id, summary in model_summaries.items():
                successful_tests = summary.get('successful_tests', 0)
                api_errors = summary.get('api_error_tests', 0)
                has_issues = summary.get('has_compatibility_issues', False)
                is_significant = summary.get('is_statistically_significant', True)

                if successful_tests >= 10 and is_significant and not has_issues and api_errors <= successful_tests:
                    valid_models.append((model_id, summary))
                else:
                    problematic_models.append((model_id, summary))

            # Sort valid models by leak percentage, problematic models by successful test count
            valid_models.sort(key=lambda x: x[1].get('leak_percentage', 0))
            problematic_models.sort(key=lambda x: x[1].get('successful_tests', 0), reverse=True)

            sorted_models = valid_models + problematic_models

        except (KeyError, TypeError) as e:
            print(f"Error sorting models: {e}")
            sorted_models = list(model_summaries.items())

        return render_template('multi_report.html',
                             data=data,
                             comparison_data=comparison_data,
                             leak_chart=leak_chart,
                             categories_chart=categories_chart,
                             sorted_models=sorted_models,
                             filename=filename)
    except (json.JSONDecodeError, KeyError) as e:
        return f"JSON error loading report: {e}", 400
    except (OSError, IOError) as e:
        return f"File error loading report: {e}", 404
    except Exception as e:
        print(f"Unexpected error in multi_report: {e}")
        return f"Error loading report: {e}", 500

@app.route('/api/reports')
def api_reports():
    """API endpoint to get all reports as JSON."""
    reports = load_reports()
    return jsonify(reports)

@app.route('/compare')
def compare_reports():
    """Compare multiple reports side by side."""
    reports = load_reports()

    # Get selected report IDs from query params
    selected_single = request.args.getlist('single')
    selected_multi = request.args.getlist('multi')

    comparison_data = {
        'single_reports': [],
        'multi_reports': []
    }

    # Load selected single model reports
    for filename in selected_single:
        try:
            if not os.path.exists(filename):
                continue
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
                data['filename'] = filename
                comparison_data['single_reports'].append(data)
        except Exception as e:
            print(f"Error loading {filename}: {e}")

    # Load selected multi-model reports
    for filename in selected_multi:
        try:
            if not os.path.exists(filename):
                continue
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
                data['filename'] = filename
                comparison_data['multi_reports'].append(data)
        except Exception as e:
            print(f"Error loading {filename}: {e}")

    return render_template('compare.html',
                         reports=reports,
                         comparison_data=comparison_data)

@app.route('/refresh')
def refresh_reports():
    """Refresh reports and redirect to index."""
    return redirect(url_for('index'))

if __name__ == '__main__':
    print("ðŸŒ Starting Prompt Injection Report Server...")
    print("ðŸ“Š Available at: http://localhost:5000")
    print("ðŸ”„ Reports will be auto-loaded from current directory")

    # Check if we have any reports
    reports = load_reports()
    total_reports = len(reports['single_model']) + len(reports['multi_model'])
    print(f"ðŸ“ Found {total_reports} reports ({len(reports['single_model'])} single, {len(reports['multi_model'])} multi)")

    if total_reports == 0:
        print("âš ï¸  No reports found. Run mistral_prompt_injection_secret_to_tool.py first to generate reports.")

    app.run(debug=True, host='0.0.0.0', port=5000)
